{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarun Nadipalli - 705.603 Creating AI-Enabled Systems Final Project\n",
    "\n",
    "For my final project, I decided to build my own content-based song recommendation application with the Spotify API. Users are able to input a link to a Spotify playlist that contains songs they like and my application will send back a link to playlist that contains my recommended songs. This guide focuses on how Spotify's Million Playlist Dataset was used to collect hundreds of thousangs of songs that will act as the pool of potential recommendations.\n",
    "\n",
    "### Data Collection Module (`data.py`) Guide\n",
    "\n",
    "This Jupyter notebook outlines the functionality built in the `data.py` script that allows us to collect millions of unique song ID's from Spotify's Million Playlist Dataset and then sequentially use the Spotify API to gather audio feature data for all of those songs. This script is what was used to collect all of the data necessary for the recommendation model. \n",
    "\n",
    "#### Spotify Million Playlist Dataset\n",
    "To begin, we must first discuss the Spotify Million Playlist Dataset, and how / why we are using it.\n",
    "\n",
    "**Background**\n",
    "\n",
    "As part of RecSys 2018 (annual Conference on Recommender Systems), Spotify released a dataset that contains 1 million user created playlists (made between 2010 and 2017) sampled from their 4 billion total playlists at the time. They released this dataset for the following challenge: Given a seed playlist title and initial set of tracks in a playlist, predict the subsequent tracks in a playlist. \n",
    "\n",
    "**Dataset Structure**\n",
    "\n",
    "The Spotify Million Playlist Dataset (MPD) is sampled from 4 billion public playlists on Spotify, contains 1 million playlists that have over 2 million unique songs and 300,000 unique artists. Each playlist in MPD contains playlist title, track list of IDs, and other metadata about the playlist. Note that:\n",
    "\n",
    "> \"Playlists are sampled with some randomization, are manually filtered for playlist quality and to remove offensive content, and have some dithering and fictitious tracks added to them. As such, the dataset is not representative of the true distribution of playlists on the Spotify platform, and must not be interpreted as such in any research or analysis performed on the dataset.\" per [AICrowd](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge)\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Here is an example of a playlist json object in the MPD:</summary>\n",
    "\n",
    "``` json\n",
    "{\n",
    "        \"name\": \"musical\",\n",
    "        \"collaborative\": \"false\",\n",
    "        \"pid\": 5,\n",
    "        \"modified_at\": 1493424000,\n",
    "        \"num_albums\": 7,\n",
    "        \"num_tracks\": 12,\n",
    "        \"num_followers\": 1,\n",
    "        \"num_edits\": 2,\n",
    "        \"duration_ms\": 2657366,\n",
    "        \"num_artists\": 6,\n",
    "        \"tracks\": [\n",
    "            {\n",
    "                \"pos\": 0,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:7vqa3sDmtEaVJ2gcvxtRID\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Finalement\",\n",
    "                \"album_uri\": \"spotify:album:2KrRMJ9z7Xjoz1Az4O6UML\",\n",
    "                \"duration_ms\": 166264,\n",
    "                \"album_name\": \"Dancing Chords and Fireflies\"\n",
    "            },\n",
    "            {\n",
    "                \"pos\": 1,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:23EOmJivOZ88WJPUbIPjh6\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Betty\",\n",
    "                \"album_uri\": \"spotify:album:3lUSlvjUoHNA8IkNTqURqd\",\n",
    "                \"duration_ms\": 235534,\n",
    "                \"album_name\": \"Endless Smile\"\n",
    "            },\n",
    "            {\n",
    "                \"pos\": 2,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:1vaffTCJxkyqeJY7zF9a55\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Some Beat in My Head\",\n",
    "                \"album_uri\": \"spotify:album:2KrRMJ9z7Xjoz1Az4O6UML\",\n",
    "                \"duration_ms\": 268050,\n",
    "                \"album_name\": \"Dancing Chords and Fireflies\"\n",
    "            },\n",
    "            // 8 tracks omitted\n",
    "            {\n",
    "                \"pos\": 11,\n",
    "                \"artist_name\": \"Mo' Horizons\",\n",
    "                \"track_uri\": \"spotify:track:7iwx00eBzeSSSy6xfESyWN\",\n",
    "                \"artist_uri\": \"spotify:artist:3tuX54dqgS8LsGUvNzgrpP\",\n",
    "                \"track_name\": \"Fever 99\\u00b0\",\n",
    "                \"album_uri\": \"spotify:album:2Fg1t2tyOSGWkVYHlFfXVf\",\n",
    "                \"duration_ms\": 364320,\n",
    "                \"album_name\": \"Come Touch The Sun\"\n",
    "            }\n",
    "        ],\n",
    "\n",
    "    }\n",
    "```\n",
    "</details>\n",
    "\n",
    "All of the million playlists are contained in over 1,000 json files (slices), and each json file contains 1,000 playlists. Our effort with this script is to iterate over these files and playlists and extract all of the unique songs and their audio features.\n",
    "\n",
    "**Why**\n",
    "\n",
    "The reason I chose to use this dataset as a pool of songs to recommend from is for a few different reasons. \n",
    "1. This is a clean dataset that already contains around 2 million unique song IDs. I wouldn't have to look elsewhere online or find large playlists in Spotify myself. I could never personally achieve that large of a dataset within the time frame of this project.\n",
    "   \n",
    "2. Having the song IDs already saves me tons of resources in terms of read / writes from the Spotify API to my local database.\n",
    "3. As per the quote above from AICrowd, this dataset is filtered to have songs of quality. Anyone, after all, can upload songs to Spotify, and although they may be good songs, I only want to recommend songs that are validated by others (since users actively added these to playlists) to ensure cleanliness of the songs I recommend.\n",
    "4. As a follow up to #3, this dataset is by no means representative of the true distribution of songs in Spotify. However, this project is more so about the recommendations, rather than the pool of songs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "Before I begin describing the methods and their functionality, it must be noted that some of the methods are based off of the work done be @nsanka on Github [here](https://github.com/nsanka/RecSys/blob/main/code/read_spotify_million_playlists.py). This person found intelligent ways to speed up the processing of this large dataset for which my Macbook is thankful (especially since Google Chrome takes up all my RAM).\n",
    "\n",
    "Note: As this is a guide, I will only be showing simple examples of how these methods work. For the actual project data collection, I have run the entire script locally and collected 300,000 songs to work upon (2 million was too much for my computer)\n",
    "\n",
    "##### create_connection(db_file)\n",
    "The first method we will look at is `create_connection(db_file)` which allows us to connect to a SQLite local file database. SQLite is a self-contained file-based SQL database that comes bundled with Python and has greater read / write efficiency compared to a .csv file or .json file. Here we instantiate a .db file that SQLite can then create schemas, tables and store data within. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connection to ./temp.db is successful!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlite3.Connection object at 0x7fbb0d4e95d0>\n"
     ]
    }
   ],
   "source": [
    "from data import *\n",
    "\n",
    "zip_file = './data/spotify_million_playlist_dataset.zip'\n",
    "db_file = './temp.db'\n",
    "\n",
    "conn = create_connection(db_file)\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create_table(conn, create_table_sql, table_name)\n",
    "\n",
    "The second method lets us use the SQLite connection object we previously made to connect to the temp.db file and create tables within. For our use case, we will create two tables: tracks and features. The tracks table will contain all of the unique song IDs and the features table will contain the audio features from Spotify for each of those song IDs. \n",
    "\n",
    "This method takes in three parameters: SQLite connection object, the table name string and the create_table_sql query string. This query string is a SQL query that defines the table schema for us so we can instantiate the table properly. \n",
    "\n",
    "The create_table_sql query is passed in through the next method:\n",
    "\n",
    "#### create_all_tables(conn)\n",
    "\n",
    "This function uses the `create_table(conn, create_table_sql, table_name)` method to create the tracks and features tables in our temp.db file. The create_table_sql queries are as below:\n",
    "\n",
    "<details>\n",
    "<summary>Create Table SQL Query for Tracks table:</summary>\n",
    "\n",
    "``` mysql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS tracks (\n",
    "track_uri text NOT NULL,\n",
    "track_id integer NOT NULL\n",
    ");\n",
    "\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Create Table SQL Query for Features table:</summary>\n",
    "\n",
    "``` mysql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS features (\n",
    "track_id integer,\n",
    "track_uri text NOT NULL,\n",
    "danceability real,\n",
    "energy real,\n",
    "key real,\n",
    "loudness real,\n",
    "mode real,\n",
    "speechiness real,\n",
    "acousticness real,\n",
    "instrumentalness real,\n",
    "liveness real,\n",
    "valence real,\n",
    "tempo real,\n",
    "duration_ms integer,\n",
    "time_signature integer\n",
    ");\n",
    "\n",
    "```\n",
    "</details>\n",
    "\n",
    "Now, let's go ahead and instantiate the tables in our temp.db SQLite database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created table tracks successfully!\n",
      "INFO:root:Created table features successfully!\n"
     ]
    }
   ],
   "source": [
    "create_all_tables(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that this works, let's run our own query using the `conn` object on our tables to the schema of the tables we've made. As we can see below, it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks Table Schema:  [(0, 'track_uri', 'TEXT', 1, None, 0), (1, 'track_id', 'INTEGER', 1, None, 0)]\n",
      "Features Table Schema:  [(0, 'track_id', 'INTEGER', 0, None, 0), (1, 'track_uri', 'TEXT', 1, None, 0), (2, 'danceability', 'REAL', 0, None, 0), (3, 'energy', 'REAL', 0, None, 0), (4, 'key', 'REAL', 0, None, 0), (5, 'loudness', 'REAL', 0, None, 0), (6, 'mode', 'REAL', 0, None, 0), (7, 'speechiness', 'REAL', 0, None, 0), (8, 'acousticness', 'REAL', 0, None, 0), (9, 'instrumentalness', 'REAL', 0, None, 0), (10, 'liveness', 'REAL', 0, None, 0), (11, 'valence', 'REAL', 0, None, 0), (12, 'tempo', 'REAL', 0, None, 0), (13, 'duration_ms', 'INTEGER', 0, None, 0), (14, 'time_signature', 'INTEGER', 0, None, 0)]\n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "query = cur.execute(\n",
    "\"\"\"\n",
    "PRAGMA table_info('tracks');   \n",
    "\"\"\" \n",
    ")\n",
    "results = query.fetchall()\n",
    "print(\"Tracks Table Schema: \", results)\n",
    "\n",
    "query = cur.execute(\n",
    "\"\"\"\n",
    "PRAGMA table_info('features');   \n",
    "\"\"\" \n",
    ")\n",
    "results = query.fetchall()\n",
    "print(\"Features Table Schema: \", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract_mpd_dataset(zip_file, num_files)\n",
    "\n",
    "This method was adapted from @nsanka on GitHub. As mentioned previously, the MPD dataset is a zip file of 1000 json files. The most efficient time/storage method of extracting the json data from these files is to use the zipfile, fnmatch, and json libraries. With these modules, we can iterate through all the files in the zip, find only the json files, and dump all of the json data into our database or process it from there. The best part of this solution is that we do not need to unzip the MPD zip file to get all of the data, greatly reducing the storage needed!\n",
    "\n",
    "The parameters for this function are the path to the MPD zip file and the number of files we would like to process out of the 1000. This function doesn't return anything and instead processes each json file iteratively - to show how it works I will run a modification of the code included in this function in the data.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Obtained all .json files from MPD.zip!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name:  data/mpd.slice.0-999.json\n",
      "Sample Data:  {'pos': 0, 'artist_name': 'Missy Elliott', 'track_uri': 'spotify:track:0UaMYEvWZi0ZqiDOoHU3YI', 'artist_uri': 'spotify:artist:2wIVse2owClT7go1WT98tk', 'track_name': 'Lose Control (feat. Ciara & Fat Man Scoop)', 'album_uri': 'spotify:album:6vV5UrXcfyQD1wu4Qo2I9K', 'duration_ms': 226863, 'album_name': 'The Cookbook'} \n",
      "\n",
      "File Name:  data/mpd.slice.1000-1999.json\n",
      "Sample Data:  {'pos': 0, 'artist_name': 'Original Broadway Cast - The Little Mermaid', 'track_uri': 'spotify:track:5IbCV9Icebx8rR6wAp5hhP', 'artist_uri': 'spotify:artist:3TymzPhJTMyupk7P5xkahM', 'track_name': 'Fathoms Below - Broadway Cast Recording', 'album_uri': 'spotify:album:3ULJeOMgroG27dpn27MDfS', 'duration_ms': 154506, 'album_name': 'The Little Mermaid: Original Broadway Cast Recording'} \n",
      "\n",
      "File Name:  data/mpd.slice.2000-2999.json\n",
      "Sample Data:  {'pos': 0, 'artist_name': 'The Jackson 5', 'track_uri': 'spotify:track:6cb0HzFQPN4BGADOmSzPCw', 'artist_uri': 'spotify:artist:2iE18Oxc8YSumAU232n4rW', 'track_name': 'ABC', 'album_uri': 'spotify:album:4GuzZh2dtsOjG3sMkx52eR', 'duration_ms': 174866, 'album_name': 'ABC'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we will test this function on the first three json files in MPD\n",
    "def extract_mpd_dataset_modified(zip_file, db_file, num_files=3):\n",
    "    with ZipFile(zip_file) as zipfiles:\n",
    "        file_list = zipfiles.namelist()\n",
    "        \n",
    "        json_files = fnmatch.filter(file_list, \"*.json\")\n",
    "        json_files = [f for i,f in sorted([(int(filename.split('.')[2].split('-')[0]), filename) for filename in json_files])]\n",
    "        logging.info('Obtained all .json files from MPD.zip!')\n",
    "        cnt = 0\n",
    "        for filename in json_files:\n",
    "            cnt += 1\n",
    "            with zipfiles.open(filename) as json_file:\n",
    "                json_data = json.loads(json_file.read())\n",
    "                # process_json_data(json_data, db_file)\n",
    "                # process json data directly here ^\n",
    "                \n",
    "                print(\"File Name: \",filename)\n",
    "                # print the first track of the first playlist in each file\n",
    "                print(\"Sample Data: \",json_data['playlists'][0]['tracks'][0], \"\\n\")\n",
    "\n",
    "            if (cnt == num_files) and (num_files > 0):\n",
    "                break\n",
    "            \n",
    "extract_mpd_dataset_modified(zip_file, db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_json_data(json_data)\n",
    "\n",
    "As we can see from the `extract_mpd_dataset()` method above, we iterate through all the files in the zip and process the json data using this function. Ultimately, this function retrieves all of the unique song IDs from the playlist and places them inside the 'tracks' table in our database. \n",
    "\n",
    "This function follows the following workflow:\n",
    "1. Get all the song IDs passed in from the extract_mpd_dataset() function\n",
    "2. Perform a left join on song IDs existing in the 'tracks' table, and the new song IDs obtained in #1\n",
    "   - The left join lets us see which song IDs are unique, new, and can be added to the table\n",
    "3. Create a temp column to determine new songs to be added to the table based on the left join in #2\n",
    "4. Delete any duplicates among the new songs to be added\n",
    "5. Drop any unnecessary data columns (only keeping the song ID and the song index for later)\n",
    "6. Add the new songs to the 'tracks' table\n",
    "\n",
    "Now, let's run the modified `extract_mpd_dataset()` function that will process the data and paste it in the 'tracks' table where we can see all of our unique song IDs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Obtained all .json files from MPD.zip!\n",
      "INFO:root:Connection to ./temp.db is successful!\n",
      "INFO:root:Number of tracks that already exist: 0\n",
      "INFO:root:Total unique tracks: 34443\n",
      "INFO:root:Adding tracks to database: 1 to 34443\n",
      "INFO:root:Connection to ./temp.db is successful!\n",
      "INFO:root:Number of tracks that already exist: 12734\n",
      "INFO:root:Total unique tracks: 23441\n",
      "INFO:root:Adding tracks to database: 34444 to 57884\n",
      "INFO:root:Connection to ./temp.db is successful!\n",
      "INFO:root:Number of tracks that already exist: 16258\n",
      "INFO:root:Total unique tracks: 18623\n",
      "INFO:root:Adding tracks to database: 57885 to 76507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks Table:  [('0UaMYEvWZi0ZqiDOoHU3YI', 2209), ('6I9VzXrHxO9rA9A5euc8Ak', 27709), ('0WqIKmW4BTrj3eJFmnCKMv', 2383), ('1AWQoqb9bSvzTjaLralEkT', 5177), ('1lzr43nnXAijIGYnCT8M8H', 7784), ('0XUfyU2QviPAs6bxSpXYG4', 2437), ('68vgtRHr7iZHpzGpon6Jlo', 27044), ('3BxWKCI06eQ5Od8TY2JBeA', 13943), ('7H6ev70Weq6DdpZyyTmUXk', 32180), ('2PpruBYCo4H7WOBJ7Q2EwM', 10585)]\n"
     ]
    }
   ],
   "source": [
    "extract_mpd_dataset(zip_file, db_file, 3)\n",
    "\n",
    "cur = conn.cursor()\n",
    "query = cur.execute(\n",
    "\"\"\"\n",
    "SELECT * FROM 'tracks' LIMIT 10;\n",
    "\"\"\" \n",
    ")\n",
    "results = query.fetchall()\n",
    "\n",
    "# The second value in each object returned is the Track Index (not a count metric)\n",
    "print(\"Tracks Table: \", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_max_track_id(conn, table_name)\n",
    "\n",
    "This is a simple helper function that returns the max track ID (meaning index) so that we can correctly input new data at the correct next empty index inside the table and keep track of how many songs we have in our database. For example, after running the process_json_data() function above, we can now see that the tracks table has a max track id of 76507. Meaning that there are 76507 unique song IDs in the tracks table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76507\n"
     ]
    }
   ],
   "source": [
    "print(get_max_track_id(conn, 'tracks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create_audio_features(conn, cnt_uris, max_songs)\n",
    "\n",
    "The last (and most important) function is `create_audio_features()`. This method takes all of unique song IDs in the tracks table and queries the Spotify API for the songs' audio features. This method is pretty simple in that it takes 100 song IDs at a time, requests the Spotify API for their features, removes null responses, and adds the important columns to the 'features' table in the database. The features we are keeping are as follows:\n",
    "Danceability, Energy, Key, Loudness, Mode, Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Tempo, Duration, and Time Signature. The full definitions and analysis of this data will be covered in the data_analysis.ipynb notebook in this project repository.\n",
    "\n",
    "The parameters are the connection object, cnt_uris is how many song IDs we include in each Spotify API call, and the max_songs parameter is a limit as to how many songs we want to put into our features table. For purposes of this example, we will only put 100 song features in our 'features' table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connection to ./temp.db is successful!\n",
      "INFO:root:Minimum Track ID in Features: 0 | Max Track ID in Tracks: 76507\n",
      "INFO:root:Getting audio features for Track ID: 1 to 100\n",
      "INFO:root:Retrieved track audio features!\n",
      "INFO:root:Connection to ./temp.db is successful!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Table:  [(1, '00z4wF0iJsp6GwDkQxkGs6', 0.737, 0.818, 8.0, -2.892, 1.0, 0.0492, 0.0936, 0.0, 0.0577, 0.893, 116.93, 232859, 4), (2, '00cSN1TMyHZErfyJbLMB05', 0.814, 0.481, 7.0, -7.892, 1.0, 0.0295, 0.345, 0.41, 0.0693, 0.834, 110.002, 208813, 4), (3, '01A7PEPSnmtixFPfB2UTal', 0.793, 0.631, 11.0, -6.109, 0.0, 0.0998, 0.0406, 0.000478, 0.207, 0.231, 119.971, 226693, 4), (4, '00BuKLSAFkaEkaVAgIMbeA', 0.825, 0.832, 5.0, -5.853, 0.0, 0.0403, 0.00587, 0.000789, 0.114, 0.713, 122.021, 220627, 4), (5, '00rrluZUPNbfTSWvodUZbV', 0.513, 0.546, 5.0, -5.703, 1.0, 0.0282, 0.0181, 0.0, 0.122, 0.245, 141.876, 246560, 4), (6, '00qOE7OjRl0BpYiCiweZB2', 0.357, 0.653, 9.0, -5.554, 1.0, 0.0654, 0.0828, 0.0, 0.0844, 0.522, 176.647, 259800, 4), (7, '00LfFm08VWeZwB0Zlm24AT', 0.662, 0.748, 5.0, -3.041, 0.0, 0.268, 0.688, 8.43e-06, 0.0841, 0.535, 82.331, 239027, 4), (8, '019FM2BxcPE2vyGWeOWhvS', 0.409, 0.602, 0.0, -7.025, 0.0, 0.108, 0.0974, 0.0, 0.107, 0.186, 110.463, 243493, 4), (9, '01KdLUbWrTXdviMq7bDhip', 0.672, 0.711, 1.0, -5.492, 0.0, 0.337, 0.0793, 0.0, 0.76, 0.518, 140.058, 261213, 4), (10, '00pgvR1zUVYubZpdY7jryZ', 0.681, 0.618, 1.0, -7.952, 0.0, 0.108, 0.063, 6.34e-05, 0.0451, 0.167, 129.823, 108760, 4)]\n"
     ]
    }
   ],
   "source": [
    "conn = create_connection(db_file)\n",
    "create_audio_features(conn, cnt_uris=100, max_songs=100)\n",
    "\n",
    "conn = create_connection(db_file)\n",
    "cur = conn.cursor()\n",
    "query = cur.execute(\n",
    "\"\"\"\n",
    "SELECT * FROM 'features' LIMIT 10;\n",
    "\"\"\" \n",
    ")\n",
    "results = query.fetchall()\n",
    "print(\"Features Table: \", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_table_df(conn, table_name, limit)\n",
    "\n",
    "The last function simply returns the data in our tables as a pandas DataFrame for use by our recommendation algorithm. It takes in the db connection object, the table we want to export, and the limit or number of rows we want to populate into our DataFrame (string parameter since we want to put it in the sql query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading table features from database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_uri</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>00z4wF0iJsp6GwDkQxkGs6</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.818</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-2.892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.09360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.893</td>\n",
       "      <td>116.930</td>\n",
       "      <td>232859</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>00cSN1TMyHZErfyJbLMB05</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.481</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-7.892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.34500</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.834</td>\n",
       "      <td>110.002</td>\n",
       "      <td>208813</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>01A7PEPSnmtixFPfB2UTal</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.631</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-6.109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.04060</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>0.231</td>\n",
       "      <td>119.971</td>\n",
       "      <td>226693</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>00BuKLSAFkaEkaVAgIMbeA</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.832</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.00587</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.713</td>\n",
       "      <td>122.021</td>\n",
       "      <td>220627</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>00rrluZUPNbfTSWvodUZbV</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.546</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.01810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.245</td>\n",
       "      <td>141.876</td>\n",
       "      <td>246560</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>00qOE7OjRl0BpYiCiweZB2</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.653</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-5.554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.08280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>0.522</td>\n",
       "      <td>176.647</td>\n",
       "      <td>259800</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>00LfFm08VWeZwB0Zlm24AT</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.748</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2680</td>\n",
       "      <td>0.68800</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.535</td>\n",
       "      <td>82.331</td>\n",
       "      <td>239027</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>019FM2BxcPE2vyGWeOWhvS</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.186</td>\n",
       "      <td>110.463</td>\n",
       "      <td>243493</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>01KdLUbWrTXdviMq7bDhip</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3370</td>\n",
       "      <td>0.07930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.518</td>\n",
       "      <td>140.058</td>\n",
       "      <td>261213</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>00pgvR1zUVYubZpdY7jryZ</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.06300</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.167</td>\n",
       "      <td>129.823</td>\n",
       "      <td>108760</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id               track_uri  danceability  energy   key  loudness  \\\n",
       "0         1  00z4wF0iJsp6GwDkQxkGs6         0.737   0.818   8.0    -2.892   \n",
       "1         2  00cSN1TMyHZErfyJbLMB05         0.814   0.481   7.0    -7.892   \n",
       "2         3  01A7PEPSnmtixFPfB2UTal         0.793   0.631  11.0    -6.109   \n",
       "3         4  00BuKLSAFkaEkaVAgIMbeA         0.825   0.832   5.0    -5.853   \n",
       "4         5  00rrluZUPNbfTSWvodUZbV         0.513   0.546   5.0    -5.703   \n",
       "5         6  00qOE7OjRl0BpYiCiweZB2         0.357   0.653   9.0    -5.554   \n",
       "6         7  00LfFm08VWeZwB0Zlm24AT         0.662   0.748   5.0    -3.041   \n",
       "7         8  019FM2BxcPE2vyGWeOWhvS         0.409   0.602   0.0    -7.025   \n",
       "8         9  01KdLUbWrTXdviMq7bDhip         0.672   0.711   1.0    -5.492   \n",
       "9        10  00pgvR1zUVYubZpdY7jryZ         0.681   0.618   1.0    -7.952   \n",
       "\n",
       "   mode  speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "0   1.0       0.0492       0.09360          0.000000    0.0577    0.893   \n",
       "1   1.0       0.0295       0.34500          0.410000    0.0693    0.834   \n",
       "2   0.0       0.0998       0.04060          0.000478    0.2070    0.231   \n",
       "3   0.0       0.0403       0.00587          0.000789    0.1140    0.713   \n",
       "4   1.0       0.0282       0.01810          0.000000    0.1220    0.245   \n",
       "5   1.0       0.0654       0.08280          0.000000    0.0844    0.522   \n",
       "6   0.0       0.2680       0.68800          0.000008    0.0841    0.535   \n",
       "7   0.0       0.1080       0.09740          0.000000    0.1070    0.186   \n",
       "8   0.0       0.3370       0.07930          0.000000    0.7600    0.518   \n",
       "9   0.0       0.1080       0.06300          0.000063    0.0451    0.167   \n",
       "\n",
       "     tempo  duration_ms  time_signature  \n",
       "0  116.930       232859               4  \n",
       "1  110.002       208813               4  \n",
       "2  119.971       226693               4  \n",
       "3  122.021       220627               4  \n",
       "4  141.876       246560               4  \n",
       "5  176.647       259800               4  \n",
       "6   82.331       239027               4  \n",
       "7  110.463       243493               4  \n",
       "8  140.058       261213               4  \n",
       "9  129.823       108760               4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = get_table_df(conn, 'features', '10')\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [Spotify Million Playlist Dataset](https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge)\n",
    "\n",
    "2. [@nsanka Github Reference](https://github.com/nsanka/RecSys/blob/main/code/read_spotify_million_playlists.py)\n",
    "\n",
    "3. [SQLite3 Documentation](https://docs.python.org/3/library/sqlite3.html)\n",
    "\n",
    "4. [SQLite3 Website](https://www.sqlite.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
