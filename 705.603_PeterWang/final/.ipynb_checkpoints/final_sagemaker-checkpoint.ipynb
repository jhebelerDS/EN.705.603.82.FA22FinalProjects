{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN.705.603.82.FA22 Creating AI-Enabled Systems<br/><br/>Stock Close Price Prediction with SageMaker\n",
    "\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "    1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "    2. [Model definitions](#Model-definitions)\n",
    "    3. [Data Setup](#Data-setup)\n",
    "3. [Training the network locally](#Training)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "    1. [Export from TensorFlow](#Export-the-model-from-tensorflow)\n",
    "    2. [Import model into SageMaker](#Import-model-into-SageMaker)\n",
    "    3. [Create endpoint](#Create-endpoint) \n",
    "5. [Validate the endpoint for use](#Validate-the-endpoint-for-use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "We will do a regression task for stock close price prediction, training locally in the box from where this notebook is being run. We then set up a real-time hosted endpoint in SageMaker.\n",
    "\n",
    "## Prequisites and Preprocessing\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services. The Sagemaker SDK will use S3 defualt buckets when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow sagemaker\n",
    "!pip install --upgrade pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker.utils import S3DataConfig\n",
    "\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "from pandas_datareader import data\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Conv1D,Flatten,MaxPooling1D,Bidirectional,LSTM,Dropout,TimeDistributed,MaxPool2D\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we'll use a CNN and LSTM network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(128, kernel_size=1, activation='relu', input_shape=(None,50,1))))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Conv1D(256, kernel_size=1, activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Conv1D(512, kernel_size=1, activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Bidirectional(LSTM(200,return_sequences=True)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Bidirectional(LSTM(200,return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='RMSprop', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the data provided by Yahoo Finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2008-01-07'\n",
    "end_date = dt.datetime.today()\n",
    "stock = data.get_data_yahoo('AMZN', start_date, end_date)\n",
    "stock.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 50\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0 , len(stock) - window_size -1 , 1):\n",
    "    first = stock.iloc[i, stock.columns.get_loc('Close')]\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for j in range(window_size):\n",
    "        temp.append((stock.iloc[i + j, stock.columns.get_loc('Close')] - first) / first)\n",
    "    temp2.append((stock.iloc[i +window_size, stock.columns.get_loc('Close')] - first) / first)\n",
    "    X.append(np.array(temp).reshape(50, 1))\n",
    "    Y.append(np.array(temp2).reshape(1,1))\n",
    "train_X,test_X,train_label,test_label = train_test_split(X, Y, test_size=0.1,shuffle=False)\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "test_X = np.array(test_X)\n",
    "train_label = np.array(train_label)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "train_X = train_X.reshape(train_X.shape[0],1,50,1)\n",
    "test_X = test_X.reshape(test_X.shape[0],1,50,1)\n",
    "\n",
    "train_labels = np.array(train_df.pop(\"class\"))\n",
    "test_labels = np.array(test_df.pop(\"class\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we train the network using the Tensorflow .fit method, just like if we were using our local computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, train_label, validation_data=(test_X,test_label), epochs=40,batch_size=64,shuffle =False)\n",
    "print(model.evaluate(test_X,test_label))\n",
    "# model.summary()\n",
    "predicted  = model.predict(test_X)\n",
    "test_label = (test_label[:,0])\n",
    "predicted = np.array(predicted[:,0]).reshape(-1,1)\n",
    "for j in range(len_t , len_t + len(test_X)):\n",
    "    temp =stock.iloc[j,stock.columns.get_loc('Close')]\n",
    "    test_label[j - len_t] = test_label[j - len_t] * temp + temp\n",
    "    predicted[j - len_t] = predicted[j - len_t] * temp + temp\n",
    "plt.plot(test_label, color = 'black', label = ' Stock Price')\n",
    "plt.plot(predicted, color = 'green', label = 'Predicted  Stock Price')\n",
    "plt.title(' Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(' Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n",
    "\n",
    "### Export the model from tensorflow\n",
    "\n",
    "In order to set up hosting, we have to import the model from training to hosting. We will begin by exporting the model from TensorFlow and saving it to our file system. We also need to convert the model into a form that is readable by ``sagemaker.tensorflow.model.TensorFlowModel``. There is a small difference between a SageMaker model and a TensorFlow model. The conversion is easy and fairly trivial. Simply move the tensorflow exported model into a directory ``export\\Servo\\`` and tar the entire directory. SageMaker will recognize this as a loadable TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"export/Servo/1\")\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new sagemaker session and upload the model on to the default S3 bucket. We can use the ``sagemaker.Session.upload_data`` method to do this. We need the location of where we exported the model from TensorFlow and where in our default bucket we want to store the model(``/model``). The default S3 bucket can be found using the ``sagemaker.Session.default_bucket`` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we upload the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_response = sm_session.upload_data(\"model.tar.gz\", bucket=bucket_name, key_prefix=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``sagemaker.tensorflow.model.TensorFlowModel`` to import the model into SageMaker that can be deployed. We need the location of the S3 bucket where we have the model and the role for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_model = TensorFlowModel(\n",
    "    model_data=f\"s3://{bucket_name}/model/model.tar.gz\",\n",
    "    role=role,\n",
    "    framework_version=\"2.3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Now the model is ready to be deployed at a SageMaker endpoint. We can use the ``sagemaker.tensorflow.model.TensorFlowModel.deploy`` method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the endpoint for use\n",
    "\n",
    "We can now use this endpoint to classify an example to ensure that it works. The output from `predict` will be an array of probabilities for each of the 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array([[[[ 0.        ],\n",
    "        [-0.01323304],\n",
    "        [ 0.01075182],\n",
    "        [-0.00729324],\n",
    "        [ 0.0112782 ],\n",
    "        [ 0.00872183],\n",
    "        [ 0.01383456],\n",
    "        [ 0.00082707],\n",
    "        [ 0.0037594 ],\n",
    "        [-0.00796991],\n",
    "        [ 0.00992487],\n",
    "        [ 0.01293234],\n",
    "        [ 0.01045112],\n",
    "        [ 0.00436092],\n",
    "        [ 0.00360899],\n",
    "        [-0.0115789 ],\n",
    "        [-0.0034587 ],\n",
    "        [-0.03872182],\n",
    "        [-0.03684206],\n",
    "        [-0.02451124],\n",
    "        [-0.02097739],\n",
    "        [-0.04624061],\n",
    "        [-0.05330824],\n",
    "        [-0.07691732],\n",
    "        [-0.06037593],\n",
    "        [-0.04172935],\n",
    "        [-0.05060153],\n",
    "        [-0.06127821],\n",
    "        [-0.06248118],\n",
    "        [-0.04278197],\n",
    "        [-0.05691729],\n",
    "        [-0.04436091],\n",
    "        [-0.04586465],\n",
    "        [-0.04624061],\n",
    "        [-0.05804512],\n",
    "        [-0.0630827 ],\n",
    "        [-0.06556392],\n",
    "        [-0.05969927],\n",
    "        [-0.07112781],\n",
    "        [-0.05345865],\n",
    "        [-0.05338345],\n",
    "        [-0.04706769],\n",
    "        [-0.04413536],\n",
    "        [-0.05180451],\n",
    "        [-0.04248121],\n",
    "        [-0.0189474 ],\n",
    "        [-0.02526316],\n",
    "        [-0.02142862],\n",
    "        [-0.00909779],\n",
    "        [-0.01909769]]]])\n",
    "predicted  = predictor.predict(sample)\n",
    "predicted[0][0] * 150 + 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all temporary directories so that we are not affecting the next run. Also, optionally delete the end points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"model.tar.gz\")\n",
    "shutil.rmtree(\"export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
